{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Clean Corpus for German Political Speeches\n",
    "\n",
    "## Overview\n",
    "\n",
    "__Goal:__ Clean messy input data and create a corpus of tokenized documents.\n",
    "\n",
    "__Input:__ CSV files with political speeches. One speech per input line.\n",
    "\n",
    "__Output:__ A _dictionary_ and a _corpus_ of documents with indexed tokens.\n",
    "\n",
    "\n",
    "## Data Preparation Steps\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "First, we need to clean the messy input data, because the raw text contains problematic punctuations and other issues which can hamper further tokenization and the quality of the resulting document models. The cleaning includes:\n",
    "\n",
    "  * removal of quotes\n",
    "  * removal of abbreviations\n",
    "  * correction of problematic word spacing (punctuation without following space)\n",
    "  * removal of all punctuation\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "A common representation of documents is a vector space model based on the words contained in the documents. Tokenization is the process for extracting 'useful' words from the documents:\n",
    "\n",
    "  * lower case conversion\n",
    "  * (sentence splitting)\n",
    "  * word splitting\n",
    "  * (building n-grams)\n",
    "  * stop word removal\n",
    "  * (lemmatization)\n",
    "\n",
    "## References\n",
    "\n",
    "* [Tutorial on Corpora and Vector Spaces](https://radimrehurek.com/gensim/tut1.html) from [Gensim](https://radimrehurek.com/gensim/index.html).\n",
    "* [Tutorial on Topic Modeling with Gensim](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) from [Machine Learning Plus](https://www.machinelearningplus.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Libraries and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "# input files\n",
    "data_dir        = '../data/'\n",
    "filename        = data_dir + 'Bundesregierung.csv'\n",
    "\n",
    "# output files\n",
    "corpus_dir      = '../corpus/'\n",
    "dict_filename   = corpus_dir + 'gps.dict'\n",
    "corpus_filename = corpus_dir + 'gps_bow.mm'\n",
    "\n",
    "# ensure output directory exists\n",
    "if not os.path.exists(corpus_dir):\n",
    "    os.makedirs(corpus_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df = pd.read_csv(filename)\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))\n",
    "\n",
    "print(len(df), 'documents imported')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Messy Input Text Data\n",
    "\n",
    "### Wrong Punctuation\n",
    "\n",
    "Examle with missing word spacing: `betreffen,Herkunftsland`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RegEx Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex pattern with greedy and look-ahead matching: '?(?=\\W)' in order to catch consecutive issues\n",
    "\n",
    "# TODO: remove URLs\n",
    "# TODO: handle compound words with '-'\n",
    "\n",
    "# remove abbreviations and ellipses\n",
    "regex_ellipsis = re.compile(r'\\.\\.\\.')\n",
    "regex_abbrev   = re.compile(r'\\s[a-z]\\.[a-zA-Z]\\.(?=\\s)')\n",
    "\n",
    "# insert missing spaces following punctuations (needed for splitting words)\n",
    "regex_comma    = re.compile(r',([^\\s\\d]{2,}?)(?=\\W)')\n",
    "regex_period   = re.compile(r'\\.([A-Z][a-z])')\n",
    "regex_sentence = re.compile(r'([\\?!):;])([^\\s])')\n",
    "\n",
    "# define punctuation to be removed\n",
    "punct_trans = str.maketrans({key:None for key in string.punctuation})\n",
    "print('punctuation to be removed:', string.punctuation)\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r'[\"–]', ' ', text)\n",
    "    text = re.sub(regex_ellipsis, ' ', text)\n",
    "    text = re.sub(regex_abbrev, '', text)\n",
    "    text = re.sub(regex_comma, r', \\1', text)\n",
    "    text = re.sub(regex_period, r'. \\1', text)\n",
    "    text = re.sub(regex_sentence, r'\\1 \\2', text)\n",
    "    text = text.translate(punct_trans)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'aus meiner Sicht, d.h. aus Sicht der operativen:Politik.Heißt,So dass Sie,zumindest hier 4,5%'\n",
    "print(test)\n",
    "clean(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pattern(pattern, texts):\n",
    "    plist = []\n",
    "    for i, doc in enumerate(texts):\n",
    "        matches = re.findall(pattern, doc)\n",
    "        if len(matches) > 0:\n",
    "            plist.append(matches)\n",
    "    items = [itm for lst in plist for itm in lst]\n",
    "    print(len(items), 'matches found, e.g.', items[0:5])\n",
    "    #print('\\n'.join(wrong))\n",
    "\n",
    "check_pattern(regex_comma, df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['text'].iloc[0][0:150], '[...]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cleaning Texts...')\n",
    "start_time = time.time()\n",
    "df['text'] = df['text'].apply(clean)\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['text'].iloc[0][0:150], '[...]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_filename = '../data/stopwords-de.txt'\n",
    "\n",
    "with open(stopwords_filename) as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "    \n",
    "#add more stopwords\n",
    "stopwords.extend('bisschen dinge dingen finde frau geehrter geehrte gerne liebe hierbei insofern manchmal möglichst punkt sicherlich sogar sozusagen vieles weiterhin zuletzt'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokenize a text and return a list of cleaned tokens.\"\"\"\n",
    "    return [word for word in text.lower().split() if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show intermediate result\n",
    "print(tokenize(df['text'].iloc[0])[0:50], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Word Frequency\n",
    "\n",
    "Remove Infrequent Tokens (Single Occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# store token frequency counts in dictionary\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "print('Counting Word Frequencies...')\n",
    "start_time = time.time()\n",
    "for doc in df['text']:\n",
    "    for token in tokenize(doc):\n",
    "        frequency[token] += 1\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))\n",
    "\n",
    "once = len([v for v in frequency.values() if v == 1])\n",
    "\n",
    "print(len(frequency), \"words in dictionary\")\n",
    "print(once, \"words with one occurrence\")\n",
    "print(len(frequency)-once, \"words with multiple occurrence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most frequent and least frequent tokens\n",
    "\n",
    "[Sort dictionary](https://docs.python.org/3/library/collections.html#ordereddict-examples-and-recipes) by token frequency in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_desc = sorted(frequency.items(), key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print('--- Most Frequent Tokens ---')\n",
    "for (k,v) in freq_desc[0:10]: print('{freq}: {token}'.format(token=k, freq=v))\n",
    "print('--- Least Frequent Tokens ---')\n",
    "for (k,v) in freq_desc[-10:]: print('{freq}: {token}'.format(token=k, freq=v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenizing Texts...')\n",
    "\n",
    "start_time = time.time()\n",
    "texts = [[token for token in tokenize(doc) if token != '' and frequency[token] > 1 ] for doc in df['text']]\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show final results\n",
    "print(texts[0][0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "print('Creating Dictionary...')\n",
    "\n",
    "start_time = time.time()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_desc = sorted(dictionary.dfs.items(), key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print('--- Most Frequent Token Occurrences in', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[0:10]: print('{freq}: {token}'.format(token=dictionary[k], freq=v))\n",
    "\n",
    "print('--- Least Frequent Token Occurrences in', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[-10:]: print('{freq}: {token}'.format(token=dictionary[k], freq=v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter extreme tokens\n",
    "\n",
    "* tokens which occur in more than 30% of all documents.\n",
    "* tokens which occur in less than 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Filtering extreme tokens')\n",
    "freq_before = len(dictionary)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.3)\n",
    "print('{} token before -> {} after'.format(freq_before, len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_desc = sorted(dictionary.dfs.items(), key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print('--- Most Frequent Token Occurrences in', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[0:10]: print('{freq}: {token}'.format(token=dictionary[k], freq=v))\n",
    "\n",
    "print('--- Least Frequent Token Occurrences in', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[-10:]: print('{freq}: {token}'.format(token=dictionary[k], freq=v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving Dictionary')\n",
    "start_time = time.time()\n",
    "dictionary.save(dict_filename)\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating Corpus')\n",
    "start_time = time.time()\n",
    "corpus_bow = [dictionary.doc2bow(doc) for doc in texts]\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving Corpus')\n",
    "start_time = time.time()\n",
    "corpora.MmCorpus.serialize(corpus_filename, corpus_bow)\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
