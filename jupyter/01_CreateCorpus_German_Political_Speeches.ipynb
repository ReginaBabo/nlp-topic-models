{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Clean Corpus for German Political Speeches\n",
    "\n",
    "## Overview\n",
    "\n",
    "__Goal:__ Cleaning the messy input data and creating a corpus of tokenized documents.\n",
    "\n",
    "__Input:__ CSV files with political speeches. One speech per input line.\n",
    "\n",
    "__Output:__ A _dictionary_ and a _corpus_ of documents with indexed tokens.\n",
    "\n",
    "\n",
    "## Data Preparation Steps\n",
    "\n",
    "### Cleaning\n",
    "\n",
    "First, we need to clean up the messy input data, because the raw text contains problematic punctuations and other issues which can hamper further tokenization and the quality of the resulting document models. The cleaning includes:\n",
    "\n",
    "  * removal of quotes\n",
    "  * handling of abbreviations\n",
    "  * handling of messy punctuation\n",
    "\n",
    "### Tokenization\n",
    "\n",
    "A common representation of documents is a vector space model based on the words contained in the documents. Tokenization is the process for extracting 'useful' words from the documents and comprises:\n",
    "\n",
    "  * lower case conversion\n",
    "  * (sentence splitting)\n",
    "  * word splitting\n",
    "  * (building n-grams)\n",
    "  * stop word removal\n",
    "  * (lemmatization)\n",
    "\n",
    "## References\n",
    "\n",
    "* [Tutorial on Corpora and Vector Spaces](https://radimrehurek.com/gensim/tut1.html) from [Gensim](https://radimrehurek.com/gensim/index.html).\n",
    "* [Tutorial on Topic Modeling with Gensim](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) from [Machine Learning Plus](https://www.machinelearningplus.com/).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Libraries and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "# input files\n",
    "data_dir        = '../data/'\n",
    "filename        = data_dir + 'Bundesregierung.csv'\n",
    "\n",
    "# output files\n",
    "corpus_dir      = '../corpus/'\n",
    "dict_filename   = corpus_dir + 'gps.dict'\n",
    "corpus_filename = corpus_dir + 'gps_bow.mm'\n",
    "\n",
    "# ensure output directory exists\n",
    "if not os.path.exists(corpus_dir):\n",
    "    os.makedirs(corpus_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df = pd.read_csv(filename)\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))\n",
    "\n",
    "print(len(df), 'speeches imported')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Messy Input Text Data\n",
    "\n",
    "### Wrong Punctuation\n",
    "\n",
    "Examle: `betreffen,Herkunftsland`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define RegEx Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split consecutive wrong punctiations with greedy and look-ahead matching: '?(?=\\W)'\n",
    "\n",
    "# TODO: remove URLs\n",
    "\n",
    "# remove abbreviations and ellipses\n",
    "regex_ellipsis = re.compile(r'\\.\\.\\.')\n",
    "regex_abbrev   = re.compile(r'\\s[a-z]\\.[a-zA-Z]\\.(?=\\s)')\n",
    "\n",
    "# insert missing spaces following punctuations (needed for splitting words)\n",
    "regex_comma    = re.compile(r',([^\\s\\d]{2,}?)(?=\\W)')\n",
    "regex_period   = re.compile(r'\\.([A-Z][a-z])')\n",
    "regex_sentence = re.compile(r'([\\?!):;])([^\\s])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define punctuation to be removed\n",
    "punct_trans = str.maketrans({key:None for key in string.punctuation})\n",
    "print('removing punctuation:', string.punctuation)\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r'[\"–]', ' ', text)\n",
    "    text = re.sub(regex_ellipsis, ' ', text)\n",
    "    text = re.sub(regex_abbrev, '', text)\n",
    "    text = re.sub(regex_comma, r', \\1', text)\n",
    "    text = re.sub(regex_period, r'. \\1', text)\n",
    "    text = re.sub(regex_sentence, r'\\1 \\2', text)\n",
    "    text = text.translate(punct_trans)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 'aus meiner Sicht, d.h. aus Sicht der operativen:Politik.Heißt,So dass Sie,zumindest hier 4,5%'\n",
    "print(test)\n",
    "print(re.findall(regex_comma, test))\n",
    "clean(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['text'].iloc[0][0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pattern(pattern, texts):\n",
    "    plist = []\n",
    "    for i, doc in enumerate(texts):\n",
    "        matches = re.findall(pattern, doc)\n",
    "        if len(matches) > 0:\n",
    "            plist.append(matches)\n",
    "    items = [itm for lst in plist for itm in lst]\n",
    "    print(len(items), 'matches found, e.g.', items[0:5])\n",
    "    #print('\\n'.join(wrong))\n",
    "\n",
    "check_pattern(regex_comma, df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df['text'] = df['text'].apply(clean)\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['text'].iloc[0][0:500], ' [...]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_filename = '../data/stopwords-de.txt'\n",
    "\n",
    "with open(stopwords_filename) as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokenize a text and return a list of cleaned tokens.\"\"\"\n",
    "    return [word for word in text.lower().split() if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show intermediate result\n",
    "print(tokenize(df['text'].iloc[0])[0:50], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Word Frequency\n",
    "\n",
    "Remove Infrequent Tokens (Single Occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# store token frequency counts in dictionary\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "start_time = time.time()\n",
    "for doc in df['text']:\n",
    "    for token in tokenize(doc):\n",
    "        frequency[token] += 1\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))\n",
    "\n",
    "once = len([v for v in frequency.values() if v == 1])\n",
    "\n",
    "print(len(frequency), \"words in dictionary\")\n",
    "print(once, \"words with one occurrence\")\n",
    "print(len(frequency)-once, \"words with multiple occurrence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most frequent and least frequent tokens\n",
    "\n",
    "[Sort dictionary](https://docs.python.org/3/library/collections.html#ordereddict-examples-and-recipes) by token frequency in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_desc = sorted(frequency.items(), key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print('--- Most Frequent Tokens ---')\n",
    "for (k,v) in freq_desc[0:10]: print('{freq}: {token}'.format(token=k, freq=v))\n",
    "print('--- Least Frequent Tokens ---')\n",
    "for (k,v) in freq_desc[-10:]: print('{freq}: {token}'.format(token=k, freq=v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "texts = [[token for token in tokenize(doc) if token != '' and frequency[token] > 1 ] for doc in df['text']]\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show final results\n",
    "print(texts[0][0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "print('Creating Dictionary')\n",
    "start_time = time.time()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving Dictionary')\n",
    "start_time = time.time()\n",
    "dictionary.save(dict_filename)\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(dictionary.iteritems())[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_desc = sorted(dictionary.dfs.items(), key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print('--- Most Frequent Token Occurrences in', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[0:25]: print('{freq}: {token}'.format(token=dictionary.id2token[k], freq=v))\n",
    "\n",
    "print('--- Least Frequent Token Occurrences in', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[-10:]: print('{freq}: {token}'.format(token=dictionary.id2token[k], freq=v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating Corpus')\n",
    "start_time = time.time()\n",
    "corpus_bow = [dictionary.doc2bow(doc) for doc in texts]\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving Corpus')\n",
    "start_time = time.time()\n",
    "corpora.MmCorpus.serialize(corpus_filename, corpus_bow)\n",
    "print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))\n",
    "\n",
    "print(corpus_bow[0:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
