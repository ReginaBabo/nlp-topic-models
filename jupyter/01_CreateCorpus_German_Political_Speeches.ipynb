{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Corpus for German Political Speeches\n",
    "\n",
    "The vector space model (VSM) is a common representation for documents in order to perfom clustering, topic modeling, classification, similarity search etc.\n",
    "\n",
    "In this case, we want to represent documents as bag-of-words. Therefore, the documents in the input text files (CSV, one document per line) are tokenized and converted into a corpus of indexed terms/tokens.\n",
    "\n",
    "### Pre-processing Steps\n",
    "\n",
    "The input data is usually messy. But instead of extensive pre-processing, e.g. cleaning of markup, punctuation, etc., we will simply extract all alphabetic sequences as tokens and nomalize them including following steps:\n",
    "\n",
    "  * convert to lower case\n",
    "  * remove stopwords\n",
    "  * create n-grams\n",
    "  * **`TODO:`** stemming\n",
    "  * **`TODO:`** lemmatization\n",
    "\n",
    "### References\n",
    "\n",
    "* [Tutorial on Corpora and Vector Spaces](https://radimrehurek.com/gensim/tut1.html) from [Gensim](https://radimrehurek.com/gensim/index.html).\n",
    "* [Tutorial on Topic Modeling with Gensim](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) from [Machine Learning Plus](https://www.machinelearningplus.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Libraries and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "# input files\n",
    "data_dir        = '../data/'\n",
    "filename        = data_dir + 'Bundesregierung.csv'\n",
    "\n",
    "# output files\n",
    "corpus_dir      = '../corpus/'\n",
    "dict_filename   = corpus_dir + 'gps_ngrams.dict'\n",
    "corpus_filename = corpus_dir + 'gps_ngrams_bow.mm'\n",
    "\n",
    "# ensure output directory exists\n",
    "if not os.path.exists(corpus_dir):\n",
    "    os.makedirs(corpus_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_diff(start_time):\n",
    "    print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))\n",
    "\n",
    "def most_frequent(tokens, topn=10):\n",
    "    frequency = defaultdict(int)\n",
    "    for doc in tokens:\n",
    "        for term in doc:\n",
    "            frequency[term] += 1\n",
    "    return sorted(frequency.items(), key=lambda t: t[1], reverse=True)[0:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df = pd.read_csv(filename)\n",
    "print_diff(start_time)\n",
    "\n",
    "print(len(df), 'documents imported')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Compound Words\n",
    "\n",
    "We want to recognize these compound words later in the n-gram detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT_COMPOUND = re.compile(r'\\w+[-]\\w+')\n",
    "\n",
    "compounds = df['text'].apply(lambda doc: [match.group() for match in PAT_COMPOUND.finditer(doc)])\n",
    "pprint(most_frequent(compounds, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "def tokens(documents):\n",
    "    \"\"\"Convert all documents into a list of lowercase tokens using Gensim's tokenize() function.\"\"\"\n",
    "    return [tokenize(doc, lower=True) for doc in documents]\n",
    "\n",
    "# explicit tokenization\n",
    "start_time = time.time()\n",
    "tokens = [[t for t in tokenize(doc, lower=True)] for doc in df['text']]\n",
    "print_diff(start_time)\n",
    "\n",
    "pprint(\" \".join(tokens[0][0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "print('Building bigrams...')\n",
    "start_time = time.time()\n",
    "bigram_model = Phrases(tokens, min_count=1, threshold=10)\n",
    "print_diff(start_time)\n",
    "\n",
    "print(bigram_model)\n",
    "\n",
    "print('Building trigrams...')\n",
    "start_time = time.time()\n",
    "bigrams = list(bigram_model[tokens])\n",
    "trigram_model = Phrases(bigrams, min_count=1, threshold=10)\n",
    "print_diff(start_time)\n",
    "\n",
    "print(trigram_model)\n",
    "\n",
    "print('Optimizing bigram/trigram models...')\n",
    "# optimize bigram, trigram models\n",
    "start_time = time.time()\n",
    "bigram_model = Phraser(bigram_model)\n",
    "trigram_model = Phraser(trigram_model)\n",
    "print_diff(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(documents):\n",
    "    return trigram_model[list(bigram_model[documents])]\n",
    "\n",
    "freq = defaultdict(int)\n",
    "\n",
    "start_time = time.time()\n",
    "for doc in n_grams(tokens):\n",
    "    for word in doc:\n",
    "        if '_' in word:\n",
    "            freq[word] += 1\n",
    "print_diff(start_time)\n",
    "\n",
    "freq_desc = sorted(freq.items(), key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, kv in enumerate(freq_desc[0:10]):\n",
    "    print('{}. {}'.format(i+1, kv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in n_grams(tokens[0:2]):\n",
    "    print(doc[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stopwords\n",
    "\n",
    "Possible stopword sources:\n",
    "  * nltk (231)\n",
    "  * https://github.com/stopwords-iso/stopwords-de (621)\n",
    "  * https://github.com/solariz/german_stopwords (1855)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_filename = '../data/stopwords-de.txt'\n",
    "\n",
    "with open(stopwords_filename) as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "    \n",
    "#add more stopwords\n",
    "stopwords.extend('anbelangt anstelle bisschen dinge dingen finde frau geehrter geehrte gerne hierbei insofern inzwischen jedenfalls lediglich liebe könnten manchmal möglichst nunmehr offensichtlich punkt selber sicherlich sogar sozusagen vieles weiterhin wiederum zuletzt'.split())\n",
    "\n",
    "stopwordsdict = dict.fromkeys(stopwords, 1)\n",
    "\n",
    "print(len(stopwordsdict), \"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords_de = stopwords.words('german')\n",
    "\n",
    "\n",
    "print(len(stopwords_de))\n",
    "stopwords_de\n",
    "\n",
    "for k in stopwords_de:\n",
    "    #print(k)\n",
    "    if k not in stopwordsdict:\n",
    "        print(k)\n",
    "\n",
    "#stopwordsdict.update(dict.fromkeys(stopwords_de, 1))\n",
    "#print(len(stopwordsdict), \"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [[word for word in doc if word not in stopwordsdict] for doc in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_nostop = remove_stopwords(tokens)\n",
    "tokens_ngram  = n_grams(tokens_nostop)\n",
    "\n",
    "print(tokens_ngram[0][0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = [[word for word in doc if '_' in word] for doc in tokens_ngram]\n",
    "\n",
    "pprint(most_frequent(ngrams, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "print('Creating Dictionary...')\n",
    "\n",
    "start_time = time.time()\n",
    "dictionary = corpora.Dictionary(tokens_ngram)\n",
    "print_diff(start_time)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_desc = sorted(dictionary.dfs.items(), key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print('--- Most Frequent Tokens in X Documents', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[0:10]: print('{freq}: {token}'.format(token=dictionary[k], freq=v))\n",
    "\n",
    "print('--- Least Frequent Tokens in X Documents', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[-10:]: print('{freq}: {token}'.format(token=dictionary[k], freq=v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter extreme tokens\n",
    "\n",
    "* tokens which occur in more than 30% of all documents.\n",
    "* tokens which occur in less than 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Filtering extreme tokens')\n",
    "freq_before = len(dictionary)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.3)\n",
    "print('{} token before -> {} after'.format(freq_before, len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_desc = sorted(dictionary.dfs.items(), key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print('--- Most Frequent Token Occurrences in', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[0:10]: print('{freq}: {token}'.format(token=dictionary[k], freq=v))\n",
    "\n",
    "print('--- Least Frequent Token Occurrences in', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[-10:]: print('{freq}: {token}'.format(token=dictionary[k], freq=v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving Dictionary to', dict_filename)\n",
    "start_time = time.time()\n",
    "dictionary.save(dict_filename)\n",
    "print_diff(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating Corpus')\n",
    "start_time = time.time()\n",
    "corpus_bow = [dictionary.doc2bow(doc) for doc in tokens_ngram]\n",
    "print_diff(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving Corpus to', corpus_filename)\n",
    "start_time = time.time()\n",
    "corpora.MmCorpus.serialize(corpus_filename, corpus_bow)\n",
    "print_diff(start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
