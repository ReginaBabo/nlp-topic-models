{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Corpus for German Political Speeches\n",
    "\n",
    "The vector space model (VSM) is a common representation for documents in order to perfom clustering, topic modeling, classification, similarity search etc.\n",
    "\n",
    "In this case, we want to represent documents as bag-of-words. Therefore, the documents in the input text files (CSV, one document per line) are tokenized and converted into a corpus of indexed terms/tokens.\n",
    "\n",
    "### Pre-processing Steps\n",
    "\n",
    "The input data is usually messy. But instead of extensive pre-processing, e.g. cleaning of markup, punctuation, etc., we will simply extract all alphabetic sequences as tokens and nomalize them including following steps:\n",
    "\n",
    "  * convert to lower case\n",
    "  * remove stopwords\n",
    "  * create n-grams\n",
    "  * (stemming - not so useful for interpreting topics)\n",
    "  * **`TODO:`** lemmatization (more complicated for german)\n",
    "\n",
    "### References\n",
    "\n",
    "* [Tutorial on Corpora and Vector Spaces](https://radimrehurek.com/gensim/tut1.html) from [Gensim](https://radimrehurek.com/gensim/index.html).\n",
    "* [Tutorial on Topic Modeling with Gensim](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/) from [Machine Learning Plus](https://www.machinelearningplus.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Libraries and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "# input files\n",
    "data_dir        = '../data/'\n",
    "filename        = data_dir + 'Bundesregierung.csv'\n",
    "\n",
    "# output files\n",
    "corpus_dir      = '../corpus/'\n",
    "dict_filename   = corpus_dir + 'gps_ngrams.dict'\n",
    "corpus_filename = corpus_dir + 'gps_ngrams_bow.mm'\n",
    "\n",
    "# ensure output directory exists\n",
    "if not os.path.exists(corpus_dir):\n",
    "    os.makedirs(corpus_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_diff(start_time):\n",
    "    print(\"--- took %d:%.2d minutes ---\" % divmod(time.time() - start_time, 60))\n",
    "\n",
    "def most_frequent(tokens, topn=10):\n",
    "    frequency = defaultdict(int)\n",
    "    for doc in tokens:\n",
    "        for term in doc:\n",
    "            frequency[term] += 1\n",
    "    return sorted(frequency.items(), key=lambda t: t[1], reverse=True)[0:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df = pd.read_csv(filename)\n",
    "print_diff(start_time)\n",
    "\n",
    "print(len(df), 'documents imported')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Compound Words\n",
    "\n",
    "We want to recognize these compound words later in the n-gram detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT_COMPOUND = re.compile(r'\\w+[-]\\w+')\n",
    "\n",
    "compounds = df['text'].apply(lambda doc: [match.group() for match in PAT_COMPOUND.finditer(doc)])\n",
    "pprint(most_frequent(compounds, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "def tokens(documents):\n",
    "    \"\"\"Convert all documents into a list of lowercase tokens using Gensim's tokenize() function.\"\"\"\n",
    "    return [tokenize(doc, lower=True) for doc in documents]\n",
    "\n",
    "# explicit tokenization\n",
    "start_time = time.time()\n",
    "tokens = [[t for t in tokenize(doc, lower=True)] for doc in df['text']]\n",
    "print_diff(start_time)\n",
    "\n",
    "pprint(\" \".join(tokens[0][0:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "print('Building bigrams...')\n",
    "start_time = time.time()\n",
    "bigram_model = Phrases(tokens, min_count=1, threshold=100)\n",
    "print_diff(start_time)\n",
    "\n",
    "print(bigram_model)\n",
    "\n",
    "print('Building trigrams...')\n",
    "start_time = time.time()\n",
    "bigrams = list(bigram_model[tokens])\n",
    "trigram_model = Phrases(bigrams, min_count=1, threshold=100)\n",
    "print_diff(start_time)\n",
    "\n",
    "print(trigram_model)\n",
    "\n",
    "print('Optimizing bigram/trigram models...')\n",
    "# optimize bigram, trigram models\n",
    "start_time = time.time()\n",
    "bigram_model = Phraser(bigram_model)\n",
    "trigram_model = Phraser(trigram_model)\n",
    "print_diff(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(documents):\n",
    "    return trigram_model[list(bigram_model[documents])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent n-grams\n",
    "start_time = time.time()\n",
    "pprint(most_frequent([[word for word in doc if '_' in word] for doc in n_grams(tokens)]))\n",
    "print_diff(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_filename = '../data/stopwords-de.txt'\n",
    "\n",
    "with open(stopwords_filename) as f:\n",
    "    stopwords = [line for line in f.read().splitlines() if not line.startswith(';')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more stopwords\n",
    "more_stopwords = 'anbelangt dingen genauso gerne hierzu hinzu liebe nahezu nunmehr punkt relativ sodass sozusagen trotz Ã¼brigen vielfach vielfache vielmehr voraussichtlich wahrlich wahrscheinlich zuvor'\n",
    "stopwords.extend(more_stopwords.split())\n",
    "\n",
    "# use dictionary for better performance\n",
    "stopwordsdict = dict.fromkeys(stopwords, 1)\n",
    "\n",
    "print(len(stopwordsdict), \"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [[word for word in doc if word not in stopwordsdict] for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.cistem import Cistem\n",
    "\n",
    "stemmer = Cistem(True)\n",
    "\n",
    "# even do stemming on each part of the n-grams\n",
    "def stemming(tokens):\n",
    "    return [['_'.join([stemmer.stem(part) for part in word.split('_')]) for word in doc] for doc in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "This is more complicated for German than English as there are fewer good algorithms available. Usually, the results are less accurate.\n",
    "\n",
    "Note: Lemmatization requires that the text has been POS tagged.\n",
    "\n",
    "  * nltk\n",
    "  * spacy\n",
    "  * pattern.de\n",
    "  * stanford NLP\n",
    "\n",
    "See also:\n",
    "\n",
    "  * https://datascience.blog.wzb.eu/2017/05/19/lemmatization-of-german-language-text/\n",
    "    * https://github.com/WZBSocialScienceCenter/germalemma\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Pre-processing together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_nostop = remove_stopwords(tokens)\n",
    "tokens_ngram  = n_grams(tokens_nostop)\n",
    "tokens_stem   = stemming(tokens_ngram)\n",
    "\n",
    "texts = tokens_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_nostop[1][0:5])\n",
    "print(tokens_ngram[1][0:5])\n",
    "print(tokens_stem[1][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent compound words after pre-processing\n",
    "ngrams = [[word for word in doc if '_' in word] for doc in tokens_ngram]\n",
    "stems  = [[word for word in doc if '_' in word] for doc in tokens_stem]\n",
    "\n",
    "pprint(most_frequent(ngrams, 10))\n",
    "pprint(most_frequent(stems, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "print('Creating Dictionary...')\n",
    "\n",
    "start_time = time.time()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print_diff(start_time)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_desc = sorted(dictionary.dfs.items(), key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print('--- Most Frequent Tokens in X Documents', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[0:10]: print('{freq}: {token}'.format(token=dictionary[k], freq=v))\n",
    "\n",
    "print('--- Least Frequent Tokens in X Documents', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[-10:]: print('{freq}: {token}'.format(token=dictionary[k], freq=v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter extreme tokens\n",
    "\n",
    "* tokens which occur in more than 30% of all documents.\n",
    "* tokens which occur in less than 5 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Filtering extreme tokens')\n",
    "freq_before = len(dictionary)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.3)\n",
    "print('{} token before -> {} after'.format(freq_before, len(dictionary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_desc = sorted(dictionary.dfs.items(), key=lambda t: t[1], reverse=True)\n",
    "\n",
    "print('--- Most Frequent Token Occurrences in', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[0:10]: print('{freq}: {token}'.format(token=dictionary[k], freq=v))\n",
    "\n",
    "print('--- Least Frequent Token Occurrences in', dictionary.num_docs, 'Documents ---')\n",
    "for (k,v) in dfs_desc[-10:]: print('{freq}: {token}'.format(token=dictionary[k], freq=v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving Dictionary to', dict_filename)\n",
    "start_time = time.time()\n",
    "dictionary.save(dict_filename)\n",
    "print_diff(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Corpus\n",
    "\n",
    "Use the n-gram tokens to construct the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating Corpus')\n",
    "start_time = time.time()\n",
    "corpus_bow = [dictionary.doc2bow(doc) for doc in texts]\n",
    "print_diff(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving Corpus to', corpus_filename)\n",
    "start_time = time.time()\n",
    "corpora.MmCorpus.serialize(corpus_filename, corpus_bow)\n",
    "print_diff(start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel, CoherenceModel\n",
    "\n",
    "num_topics = 150\n",
    "\n",
    "start_time = time.time()\n",
    "model_lda = LdaModel(corpus_bow, id2word=dictionary, num_topics=num_topics)\n",
    "print_diff(start_time)\n",
    "print(model_lda)\n",
    "\n",
    "model_lda.save('../model/{}_topics/'.format(num_topics) + 'topic_model.lda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Coherence\n",
    "\n",
    "Remove words from texts which are not in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[token for token in text if token in dictionary.token2id] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "cm = CoherenceModel(model=model_lda, corpus=corpus_bow, dictionary=dictionary, coherence='u_mass')\n",
    "print('u_mass: {:0.3f}'.format(cm.get_coherence()))\n",
    "print_diff(start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "cm = CoherenceModel(texts=texts, model=model_lda, corpus=corpus_bow, dictionary=dictionary, coherence='c_v')\n",
    "print('c_v: {:0.3f}'.format(cm.get_coherence()))\n",
    "print_diff(start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
